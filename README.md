# Natural Language Processing

## Course Information

* Materials prepared for NLP offered at University of Guilan:
    * [NLP981] 1st semester of 2019/1398
* Credits: 3.0
* Instructor: [Javad PourMostafa](https://javad.pourmostafa.com)
* TA: Parsa Abbasi
* <b>Note: Drop me a line to get the slides!</b>

## Theoretical Sessions

* ### Week 1: Introduction to NLP
  > Main Approaches (Rule-based, Probabilistic Models, Traditional ML algorithms and Neural Networks)
  > Confusion Matrix, Semantic Slot Filling, NLP Pyramid, Scenario of Text Classification, Gradient Descent
  > Tokenization, Normalization, Stemmer, Lemmatizer, BoW, N-Grams, TF-IDF, Binary Logistic Regression
  > Hashing Features for word representation, Neural Vectorization, 1-D Convolutional Layer

* ### Week 2: Language Modeling
  Chain Rule, Probability of a sequence of words, Markov assumption, Bi-gram, Maximum Likelihood Estimation (MLE)<br>
  Generative Model, Evaluating Language Models (Extrinsic, Intrinsic), Perplexity, Smoothing (discounting)<br>
  Laplace Smoorhing(Add-one, Add-k), Stupid/Katz Backoff, Kneser-Ney Smoothing

* ### Week 3: Hidden Markov Model for Sequence Labeling (POS)
  Sequence Labeling, Markov Model Scenario, Markov Chain Model, Emission and Transition Probabilities<br>
  HMM for POS tagging, Text generation in HMM, Training HMM, Viterbi Algorithm, Using Dynamic Programming for backtracing

* ### Week 4: Neural Language Models
  Curse of Dimensionality, Distributed Representation, Neuron, Activation Functions, The Perceptron<br>
  The XOR problem, Feed-Forward Neural Networks, Trainin Neural Networks, Loss Function<br>
  Cross-Entropy Loss, Droupout, A Neural Probabilistic Language Model, Recurrent Neural Languge Models<br>
  Gated Recurrent Neural Networks, LSTM
  
## Practical Sessions

* ### Week 1: Supervised Classification
  Confusion Matrix, Whisker Plot, Using Supervised Models like Logistic Regression, Decision Tree, and so on.

* ### Week 2: Introduction to Neural Networks
  Feedforward NN, Using MSE as a loss function, Updating weights in backpropagation, Gradient Descent Algorithm.

* ### Week 3: Vector Semantics and Embeddings
  Bag of Words, Finding Unique Words, Creating Document-Word Matrix, TF-IDF Computation<br> 
  Finally, investigate our naive TF-IDF model in comparison with SKlearn TfidfVectorizer

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

